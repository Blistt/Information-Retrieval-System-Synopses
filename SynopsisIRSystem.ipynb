{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4bkieQIH112",
        "outputId": "ddbed111-016d-47bf-d823-fab05d794fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBt9BtyuS8og"
      },
      "source": [
        "**Functions to read csv with animes' synopsis**\n",
        "\n",
        "\n",
        "To save computation time, when tokenization is called, priorly computed tokenized versions of synopsis are read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYdIEqWdaIob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11bcd6f1-200f-44cd-a96d-2ac39a29f2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "def reader(path, num_sim_shows=1, show=False):\n",
        "  df = pd.read_csv(path)\n",
        "  df = df[['uid', 'title', 'synopsis', 'similar_shows']]\n",
        "  if show:\n",
        "    print(df.shape, 'shows without cleaning')\n",
        "  df = df[pd.notna(df['similar_shows'])]\n",
        "  if show:\n",
        "    print(df.shape, 'shows after removing null values for similar shows')\n",
        "\n",
        "  # Discards shows without synopsis\n",
        "  df = df[pd.notna(df['synopsis'])]\n",
        "  if show:\n",
        "    print(df.shape, 'shows after removing null values for synopsis')\n",
        "  df['similar_shows'] = df['similar_shows'].str.replace(\"'\",'')\\\n",
        "\n",
        "  # Converts csv cell values from String to List\n",
        "  df['similar_shows'] = df['similar_shows'].map(lambda x: literal_eval(x))\n",
        "\n",
        "  # Removes similar shows not in dataset\n",
        "  all_shows = list(df['uid'])\n",
        "  total_sim_shows = sum(df['similar_shows'].str.len())\n",
        "  df['similar_shows'] = df['similar_shows'].map(lambda x: [i for i in x if i in all_shows])\n",
        "\n",
        "  if show:\n",
        "    print('total similar shows:', total_sim_shows)\n",
        "    print('total removed shows:', total_sim_shows - sum(df['similar_shows'].str.len()))\n",
        "\n",
        "  # Removes shows without at least 'num_sim_shows' number of similar shows\n",
        "  more_than = df['similar_shows'].map(lambda x: True if len(x) >= num_sim_shows else False)\n",
        "  df = df[more_than]\n",
        "  if show:\n",
        "    print(df.shape, 'shows with at least', num_sim_shows, 'similar shows')\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGLZn_7Q4Pea"
      },
      "source": [
        "**Creates Bag of Words (Tfidf Matrix)**\n",
        "\n",
        "This approach will benefit from tokenization, so tokenized versions of the synopses are retrieved from memory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_tfidf(path, show=True):\n",
        "  # Reads tokenized dataset of anime shows\n",
        "  df = reader(path + '/tokenized_animes_with_userrecs.csv', num_sim_shows=1)\n",
        "  if show:\n",
        "    print('Dataset with', df.shape[0], 'different shows')\n",
        "    print('\\n' + 'Sample Synopsis\\n', 'Show:', df.loc[55, 'title'] + '\\n' + df.loc[55, 'synopsis'])\n",
        "    print('----------------------------------------------------------------------------------------------------------------------------------------------------------' + '\\n')\n",
        "  return df"
      ],
      "metadata": {
        "id": "sT4GYVkMR5-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es0nOJGbmspS"
      },
      "outputs": [],
      "source": [
        "# Creates a TD-IDF matrix\n",
        "def TfidfGenerator(df, show=True):\n",
        "  synopses = list(df['synopsis'])\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  vectors = vectorizer.fit_transform(synopses)\n",
        "  feature_names = vectorizer.get_feature_names_out()\n",
        "  dense = vectors.todense()\n",
        "  denselist = dense.tolist()\n",
        "  tfidf_matrix = pd.DataFrame(denselist, columns=feature_names)\n",
        "\n",
        "  if show:\n",
        "    print('Language Encodings Head')\n",
        "    print(tfidf_matrix.head())\n",
        "    print('Shape of dataset of Language Encodings:', tfidf_matrix.shape)\n",
        "    print('----------------------------------------------------------------------------------------------------------------------------------------------------------' + '\\n')\n",
        "  return tfidf_matrix.values, df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W48aOPUhrIXt"
      },
      "source": [
        "**Creates Dynamic Embedding using a Pre-Trained Universal Sentence Encoder (USE)**\n",
        "\n",
        "\n",
        "This method peforms best without performing tokenization, so untokenized version of shows synopses are retrieved from memory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_USE(path, show=True):\n",
        "  # Reads untokenized dataset of anime shows\n",
        "  df = reader(path +'/animes_with_userrecs_cleaned.csv', num_sim_shows=1)\n",
        "  df['synopsis'] = df['synopsis'].replace(r'\\r\\n', '', regex=True)\n",
        "  df['synopsis'] = df['synopsis'].str.replace('  [Written by MAL Rewrite]', '', regex=False)\n",
        "  df['synopsis'] = df['synopsis'].str.split('.')\n",
        "  df['synopsis'] = df['synopsis'].apply(lambda x: [y for y in x if y != ''])\n",
        "  if show:\n",
        "    print('Dataset with', df.shape[0], 'different shows')\n",
        "    print('\\n', 'Sample Synopsis\\n', 'Show:', df.loc[55, 'title'])\n",
        "    print(df.loc[55, 'synopsis'])\n",
        "    print('----------------------------------------------------------------------------------------------------------------------------------------------------------' + '\\n')\n",
        "  return df"
      ],
      "metadata": {
        "id": "90GSK9hLSS4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15hk6PyKxlZ_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n",
        "def USEGenerator(df, show=True):\n",
        "  # Downloads pre-trained USE model\n",
        "  module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "  model = hub.load(module_url)\n",
        "\n",
        "  # Creates embeddings for list of show synopses\n",
        "  synopses = list(df['synopsis'])\n",
        "  USE_matrix = []\n",
        "  for synopsis in synopses:\n",
        "    embedding = model(synopsis)[0].numpy()\n",
        "    USE_matrix.append(embedding)\n",
        "  USE_matrix = np.array(USE_matrix)\n",
        "\n",
        "  if show:\n",
        "    print('Language Encodings Head')\n",
        "    print(USE_matrix[0:5])\n",
        "    print('Shape of dataset of Language Encodings:', USE_matrix.shape)\n",
        "    print('----------------------------------------------------------------------------------------------------------------------------------------------------------' + '\\n')\n",
        "  return USE_matrix, df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khTEqWTKrND5"
      },
      "source": [
        "**Creates Training/Testing dataset**\n",
        "\n",
        "\n",
        "Given the small size of this project's datset allowed for Leave One Out-Cross Validation, the training and testing dataset are the same\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3iN03X4HaxW"
      },
      "outputs": [],
      "source": [
        "def create_dataset(test_size, df, text_encodings):\n",
        "  dataset = df[:test_size]\n",
        "  dataset = dataset.reset_index(drop=True)\n",
        "  dataset_matrix = text_encodings[:test_size]\n",
        "  if isinstance(dataset_matrix, pd.DataFrame):\n",
        "    dataset_matrix.reset_index(drop=True)\n",
        "\n",
        "  # Creates array of true relevance labels with 0s for tue negatives (non-relevant shows) and 1s for true positive (relevant shows)\n",
        "  dataset.insert(dataset.shape[1], 'true', '')\n",
        "  for i in range(dataset.shape[0]):\n",
        "    dataset.at[i, 'similar_shows'] = [df.loc[df['uid']==j].index[0] for j in dataset.at[i, 'similar_shows']]\n",
        "    trues = np.zeros(df.shape[0])\n",
        "    trues[dataset.at[i, 'similar_shows']] = 1.0\n",
        "    dataset.at[i, 'true'] = trues\n",
        "\n",
        "  # Initializes auxiliary columns for optional tasks\n",
        "  dataset.insert(dataset.shape[1], 'pred', '')\n",
        "  dataset.insert(dataset.shape[1], 'recs', '')\n",
        "  dataset.insert(dataset.shape[1], 'recs_dist', '')\n",
        "  return dataset, dataset_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYzLFHfiGHYq"
      },
      "source": [
        "**Retrieval Method**\n",
        "\n",
        "Retrieves K shows via Nearest Neighbor search with Cosine Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LABzkZTW317H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.neighbors\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "def retrieve_knn(dataset, dataset_matrix, text_encodings, n_neighbors=30, show=False):\n",
        "  # Initializes KNN algorithm\n",
        "  knnbr = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm ='brute').fit(text_encodings)\n",
        "\n",
        "\n",
        "  # The set of K Nearest Neighobrs is added as recommendations to each show in the test set\n",
        "  distances, indices = knnbr.kneighbors(dataset_matrix)\n",
        "\n",
        "  # Construct training set\n",
        "  X = []\n",
        "  for i, show_recs in enumerate(indices):\n",
        "    # Creates additional array of predictions for Mean Average Prediction Computation\n",
        "    preds = np.zeros(text_encodings.shape[0])\n",
        "    preds[show_recs] = 1\n",
        "    dataset.at[i,'pred'] = preds\n",
        "\n",
        "    # Creates query-document pairings as entires from retrieved shows, each with ids, relevance prediction, distances,\n",
        "    # and ground truth labels\n",
        "    for j, rec in enumerate(show_recs[1:]):\n",
        "      entry = [i, dataset.loc[i, 'synopsis'], rec, 1, distances[i][j+1], int(rec in dataset.loc[i, 'similar_shows'])]\n",
        "      X.append(entry)\n",
        "\n",
        "    # Creates query-document pairiings as entires from relevant but non-retrieved shows each with ids, relevance prediction,\n",
        "    # distances, and ground truth labels\n",
        "    sims = [x for x in dataset.loc[i, 'similar_shows'] if x not in show_recs]\n",
        "    for j, true in enumerate(sims):\n",
        "      distance = cosine_distances([text_encodings[i]], [text_encodings[j]])\n",
        "      entry = [i, dataset.loc[i, 'synopsis'], true, 0, distance, 1]\n",
        "      X.append(entry)\n",
        "  X = pd.DataFrame(X)\n",
        "  X.columns = ['doc_id', 'query', 'show_id', 'pred', 'distance', 'true']\n",
        "\n",
        "  if show:\n",
        "    print('Constructed training set with', X.shape[0], 'query-document pairings')\n",
        "    print(X.head())\n",
        "\n",
        "  return X, dataset, knnbr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjmoj5zz0N7A"
      },
      "source": [
        "**Metrics**\n",
        "\n",
        "Computes precision, recall and the F1 measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP4xui6EJjJK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "def get_precision_recall_f1(dataset):\n",
        "  precision = precision_score(dataset['true'], dataset['pred'])\n",
        "  recall = recall_score(dataset['true'], dataset['pred'])\n",
        "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "  return precision, recall, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CGe-CbeLtYK"
      },
      "source": [
        "Mean Average Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p9szrdxZsE5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import PrecisionRecallDisplay\n",
        "\n",
        "def get_mean_average_precision(dataset, retrieval_size):\n",
        "  if retrieval_size < dataset.shape[0]-1:\n",
        "    return \"Too small retrieval size for computing MAP\"\n",
        "\n",
        "  for i in range(1):\n",
        "      dataset.loc[i, 'average_precision'] = average_precision_score(dataset.loc[i, 'true'], dataset.loc[i, 'pred'])\n",
        "\n",
        "  mean_average_precision = dataset['average_precision'].mean()\n",
        "\n",
        "  return mean_average_precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDTfvSUMJyGC"
      },
      "source": [
        "R-Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzfuqaJ2JxhW"
      },
      "outputs": [],
      "source": [
        "def get_R_Precision(dataset, retrieval_size):\n",
        "  if retrieval_size < 200:\n",
        "    return \"Too small retrieval size for computing R-Precision\"\n",
        "\n",
        "  R_Precision = []\n",
        "  # Iterate over all unique queries\n",
        "  for i in range(dataset['doc_id'].unique().shape[0]):\n",
        "    recs = dataset[(dataset['doc_id']==i) & (dataset['pred']==1)].sort_values(by='distance')\n",
        "    rels_len = dataset[(dataset['doc_id']==i) & (dataset['true']==1)].shape[0]\n",
        "    R_Precision.append(recs.iloc[:rels_len, dataset.columns.get_loc('true')].sum() / rels_len)\n",
        "\n",
        "  R_Precision = np.array(R_Precision).mean()\n",
        "  return R_Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8h2IuN8Ifpb"
      },
      "source": [
        "**Experiment**\n",
        "\n",
        "Retrieves documents for a specified retrieval size and evaluates performance of the system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftXYTBED1Oxc"
      },
      "outputs": [],
      "source": [
        "def experiment(path, retrieval_size=30, test_set_size=3147, text_encodings='tfidf'):\n",
        "\n",
        "  # Extract desired language encodings\n",
        "  if text_encodings == 'tfidf':\n",
        "    df = read_tfidf(path)\n",
        "    text_encodings, df = TfidfGenerator(df)\n",
        "  else:\n",
        "    df = read_USE(path)\n",
        "    text_encodings, df = USEGenerator(df)\n",
        "\n",
        "  # Generate training and test set\n",
        "  dataset, dataset_matrix = create_dataset(test_set_size, df, text_encodings)\n",
        "\n",
        "  # Retrieve results with KNN\n",
        "  X, map_X, knnbr = retrieve_knn(dataset, dataset_matrix, text_encodings, retrieval_size+1)\n",
        "\n",
        "  # Compute Evaluation Metrics - *Due to my sloppy implementation, R-precision and MAP can only be computed if retrieval size is 3146\n",
        "  precision, recall, f1 = get_precision_recall_f1(X)\n",
        "  mean_average_precision = get_mean_average_precision(map_X, retrieval_size)\n",
        "  R_Precision = get_R_Precision(X, retrieval_size)\n",
        "\n",
        "  print('PRECISION @ ' + str(retrieval_size) + ':',  precision)\n",
        "  print('RECALL @ ' + str(retrieval_size) + ':', recall)\n",
        "  print('F1 SCORE @ ' + str(retrieval_size) + ':', f1)\n",
        "  print('Mean Average Precision:', mean_average_precision)\n",
        "  print('R_Precision:', R_Precision)\n",
        "  print('----------------------------------------------------------------------------------------------------------------------------------------------------------' + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I9kSoii2Sow",
        "outputId": "43ad5a8b-4317-4708-a290-44e63821e994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset with 3147 different shows\n",
            "\n",
            " Sample Synopsis\n",
            " Show: Slam Dunk\n",
            "['Hanamichi Sakuragi, infamous for this temper, massive height, and fire-red hair, enrolls in Shohoku High, hoping to finally get a girlfriend and break his record of being rejected 50 consecutive times in middle school', ' His notoriety precedes him, however, leading to him being avoided by most students', ' Soon, after certain events, Hanamichi is left with two unwavering thoughts: \"I hate basketball,\" and \"I desperately need a girlfriend', '\"  One day, a girl named Haruko Akagi approaches him without any knowledge of his troublemaking and asks him if he likes basketball', ' Hanamichi immediately falls head over heels in love with her, blurting out a fervent affirmative', ' She then leads him to the gymnasium, where she asks him if he can do a slam dunk', ' In an attempt to impress Haruko, he makes the leap, but overshoots, instead slamming his head straight into the blackboard', \" When Haruko informs the basketball team's captain of Hanamichi's near-inhuman physical capabilities, he slowly finds himself drawn into the camaraderie and competition of the sport he had previously held resentment for\"]\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Language Encodings Head\n",
            "[[-0.0479993  -0.10620163  0.01119967 ... -0.07219918  0.02557949\n",
            "   0.03154479]\n",
            " [ 0.00848786 -0.06567747  0.03770744 ... -0.02749309  0.07892376\n",
            "  -0.02634676]\n",
            " [-0.0053421   0.06587225 -0.06399372 ... -0.06145347  0.07515669\n",
            "  -0.07488218]\n",
            " [ 0.0291263   0.03319269 -0.01728515 ... -0.02018322  0.07928449\n",
            "   0.01454576]\n",
            " [-0.00467441 -0.0756701   0.00904582 ...  0.00895914  0.07403173\n",
            "  -0.04596841]]\n",
            "Shape of dataset of Language Encodings: (3147, 512)\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "PRECISION @ 50: 0.027473784556720688\n",
            "RECALL @ 50: 0.06239265663111406\n",
            "F1 SCORE @ 50: 0.038149110692428864\n",
            "Mean Average Precision: Too small retrieval size for computing MAP\n",
            "R_Precision: Too small retrieval size for computing R-Precision\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text_encoding = 'USE'     # Select between 'USE' and 'tdidf'\n",
        "path = '/content/drive/MyDrive/SynopsisIRSystem'\n",
        "experiment(path, retrieval_size=50, text_encodings=text_encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OPTIONAL - Test Your Own Query**\n",
        "\n",
        "This method supports the testing of a query written by the user. Just modify the string value for the *query* value in the bottom cell"
      ],
      "metadata": {
        "id": "LDzyCTc23Jww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "import textwrap\n",
        "\n",
        "def test_your_own_query(query, path, retrieval_size, text_encodings='tfidf'):\n",
        "\n",
        "  query_df = pd.DataFrame([{'synopsis': query}])\n",
        "\n",
        "  # Extract desired language encodings\n",
        "  if text_encodings == 'tfidf':\n",
        "    df = read_tfidf(path, show=False)\n",
        "    df = pd.concat([query_df.reindex(columns=df.columns), df], ignore_index=True)\n",
        "    text_encodings, df = TfidfGenerator(df, show=False)\n",
        "  else:\n",
        "    df = read_USE(path, show=False)\n",
        "    text_encodings, df = USEGenerator(df, show=False)\n",
        "  df = reader(path+'/animes_with_userrecs_cleaned.csv', show=False)\n",
        "\n",
        "  # Get distances\n",
        "  distances = pairwise_distances(text_encodings[0].reshape(1, -1), text_encodings, metric='cosine').ravel()\n",
        "\n",
        "  # Sort and obtain K shortest distances (K Nearest Neighbors)\n",
        "  knn_indices = np.argsort(distances)[:retrieval_size]\n",
        "  knn = df.iloc[knn_indices]\n",
        "  knn['distance'] = distances[knn_indices]\n",
        "  knn = knn[1:]\n",
        "\n",
        "  for i, row in knn.iterrows():\n",
        "    print(row['title'], '  ---- distance   ', row['distance'])\n",
        "    print(textwrap.fill(row['synopsis']))\n",
        "    print('----------------------------------------------------------------------------------------------------------------------------------------------------------' + '\\n')"
      ],
      "metadata": {
        "id": "Jyru1kjFVN4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'I want vampiros and werewolves, and castles, and shotguns and a red head'\n",
        "retrieval_size = 10\n",
        "path = '/content/drive/MyDrive/SynopsisIRSystem'\n",
        "\n",
        "test_your_own_query(query, path, retrieval_size+1, text_encodings='tfidf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1sMqBGnYxDx",
        "outputId": "f11049b9-cd37-4b20-e022-2683cbfbb764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Morita-san wa Mukuchi.   ---- distance    0.9077696364670396\n",
            "Morita Mayu, a high school girl. She is extremely reticent and her\n",
            "silence and habit of looking at people's eyes straightly sometimes\n",
            "cause misunderstanding. The reason behind it is not because she\n",
            "doesn’t like to talk nor because she has nothing to say. The reason\n",
            "she rarely speaks is due to the fact she thinks too much before\n",
            "speaking, thus losing the timing to speak altogether. But she lives a\n",
            "happy school life with her classmates.\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Double Decker! Doug & Kirill   ---- distance    0.9127615817395552\n",
            "The once peaceful city-state of Lisvalletta has found itself beset by\n",
            "a dangerous new drug called Anthem. The side effects of the drug allow\n",
            "the user to enter a state of Overdrive, wherein they mutate into\n",
            "superpowered beasts with inhuman abilities. With the police powerless\n",
            "to stop this new threat, the responsibility falls upon the Special\n",
            "Crime Investigation Unit SEVEN-O. To offset the dangers of this work,\n",
            "the investigators work under the patented \"Double Decker\" system,\n",
            "which requires them to team up in \"buddy cop\" pairs.      As a child,\n",
            "average police officer Kirill Vrubel fantasized about being a hero who\n",
            "would save his school from a random terrorist attack. His chance to be\n",
            "a hero arrives when his landlady blackmails him into searching for her\n",
            "lost cat. Upon arriving and falling asleep in an abandoned warehouse,\n",
            "Kirill finds himself in the middle of a hostage situation involving an\n",
            "Anthem user. By teaming up with SEVEN-O detective Douglas \"Doug\"\n",
            "Bilingam, Kirill earns his spot as the newest member of SEVEN-O. Now,\n",
            "with the help of this secret organization, he may finally achieve his\n",
            "dream of becoming a hero.      [Written by MAL Rewrite]\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Mechano: Scientific Attack Force   ---- distance    0.9147055255657175\n",
            "Three 10-minute videos present a trippy view into the minds of their\n",
            "creators. Brought together by Pierre Taki of Denki Groove,  Mechano:\n",
            "Scientific Attack Force  features three shorts done in very different\n",
            "styles.      The three short films are:      \"Plastic Gun Man\" - a 3D\n",
            "Western spoof   \"World Meccano Triangle\" - a music video reminiscent\n",
            "of '90s era screensavers   \"Haiirogaoka no Soridaijin\" (translated as\n",
            "\"Prime Minister of Gray Hill\") - an anime-style animated video parody\n",
            "of Akira Mochizuki's famous 1977 manga,  Yuuhi ga Oka no Souri Daijin\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Concrete Revolutio: Choujin Gensou   ---- distance    0.9150045824795047\n",
            "On a sunny July day in the 41st year of the Shinka Era, Jirou\n",
            "Hitoyoshi is tasked with covertly listening in on a secret meeting\n",
            "between a top government scientist and an industrial spy. However, his\n",
            "cover is blown, and the spy reveals himself to be an alien in\n",
            "disguise. Amidst the ensuing chaos, Jirou enlists the aid of cafe\n",
            "waitress and magical girl Kikko Hoshino, one of many \"superhumans\" who\n",
            "blend into society and secretly protect humanity from extraterrestrial\n",
            "threats. As a member of the government agency known as the Super\n",
            "Population Research Laboratory, Jirou has the dual task of protecting\n",
            "superhumans that defend humanity and disposing of any deemed too\n",
            "dangerous to live. Having proven herself a worthy ally, Kikko is\n",
            "invited to join the agency as its newest recruit.      Fast forward\n",
            "five years: disapproval and distaste for superhumans are now\n",
            "commonplace in Tokyo. From government corruption and conflicting ideas\n",
            "of justice, to the morality of superhuman rights, the relationship\n",
            "between humans and the supernatural minority balances precariously in\n",
            "a world pervaded by whispers of unrest and unease. Under mysterious\n",
            "circumstances, Jirou has betrayed the agency, and is now a fugitive on\n",
            "the run. As he skulks through the rainy back alleys of Shinjuku, he is\n",
            "pursued by the very same superhumans that he himself once recruited.\n",
            "[Written by MAL Rewrite]\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Needless   ---- distance    0.9185042784643365\n",
            "At the onset of World War III, nobody could have predicted the effect\n",
            "it would have on Japan. While it had officially ended fifty years ago\n",
            "in 2150, its battles still persist. Large, mysterious areas known as\n",
            "\"Blackspots\" appeared across the country, filled with the contaminated\n",
            "ruins of cities and countrysides. Those inside were trapped to halt\n",
            "the spread of contamination, and their powers began to mutate—be they\n",
            "shapeshifters, pyromancers, or controllers of gravity itself—they all\n",
            "became known as the Needless.      Adam Blade is one such Needless,\n",
            "possessing remarkable regenerative abilities and incredible strength.\n",
            "In order to restore peace to a war-torn Japan, he and his allies must\n",
            "fight together to rise against a heinous research group by the name of\n",
            "Simeon.\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Evangelion: 3.0+1.0   ---- distance    0.9225818341757444\n",
            "The fourth and final movie of the Evangelion rebuild movie series.\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Fushigi na Somera-chan   ---- distance    0.9232741732304441\n",
            "The story follows the everyday life of Somera Nonomoto who can use the\n",
            "strongest kenpo, Nonomoto Mahou-ken, which is inherited from her\n",
            "mother with her younger sister Kukuru.      (Source: Crunchyroll)\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Tanaka-kun wa Itsumo Kedaruge   ---- distance    0.9236073523452357\n",
            "For high school student Tanaka, the act of being listless is a way of\n",
            "life. Known for his inattentiveness and ability to fall asleep\n",
            "anywhere, Tanaka prays that each day will be as uneventful as the\n",
            "last, seeking to preserve his lazy lifestyle however he can by\n",
            "avoiding situations that require him to exert himself. Along with his\n",
            "dependable friend Oota who helps him with tasks he is unable to\n",
            "accomplish, the lethargic teenager constantly deals with events that\n",
            "prevent him from experiencing the quiet and peaceful days he longs\n",
            "for.      [Written by MAL Rewrite]\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Girls Bravo: First Season   ---- distance    0.9255218542863279\n",
            "Small for his age, Yukinari has been bullied and abused by girls all\n",
            "his life. Now in high school, he has developed a rare condition:\n",
            "whenever girls touch him, or even come close, he breaks out in hives.\n",
            "Imagine his surprise, when he is suddenly transported to the city of\n",
            "Seiren on a mystic world invisibly orbiting the Earth, and populated\n",
            "with vast numbers of women and very few men. Fortunately, he has a new\n",
            "friend, Miharu-chan, whose touch inexplicably doesn't affect him.\n",
            "(Source: ANN)\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Yumekui Merry   ---- distance    0.9281970466817416\n",
            "Yumekui Merry  begins with Yumeji Fujiwara, a seemingly average high\n",
            "school student. But ten years ago, Yumeji gained the ability to see\n",
            "the dream auras of other people around him. This ability allows Yumeji\n",
            "to predict what type of dreams people are likely to have next. The\n",
            "dreams of others may not be a mystery to Yumeji, but his own dreams\n",
            "have recently left him puzzled. In dream after dream, Yumeji has been\n",
            "pursued by an army of cats lead by John Doe, who claims he needs\n",
            "Yumeji's body to enter the real world.      These strange occurrences\n",
            "get stranger when Yumeji meets Nightmare Merry, a dream demon seeking\n",
            "to return to her own world. Using his powers, Yumeji decides to assist\n",
            "Merry in getting back home. But Merry's very presence in the real\n",
            "world means that the barrier separating dreams and reality has been\n",
            "broken, and not all of the dream demons intend to come to Earth\n",
            "peacefully...\n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-c4401e44564e>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  knn['distance'] = distances[knn_indices]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}